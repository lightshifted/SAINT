{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b966e976",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import Module\n",
    "from torch import Tensor, einsum\n",
    "from torch.optim.optimizer import Optimizer\n",
    "from torch.optim.lr_scheduler import _LRScheduler as LRScheduler\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchmetrics import Metric as TorchMetric\n",
    "\n",
    "import einops\n",
    "from einops import rearrange\n",
    "\n",
    "from operations.data import generate_dataset\n",
    "from operations.data import generate_dataloader\n",
    "from operations.embeds import Embedding\n",
    "from operations.model import NewGELU\n",
    "from operations.utils import generate_splits\n",
    "from operations.utils import preprocess\n",
    "from operations.utils import CutMix, Mixup\n",
    "\n",
    "from typing import Optional, Dict, List, Tuple, Union, Any\n",
    "from argparse import Namespace\n",
    "from abc import ABC, abstractmethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef03aabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics\n",
    "class Metric(object):\n",
    "    def __init__(self):\n",
    "        self._name = \"\"\n",
    "\n",
    "    def reset(self):\n",
    "        raise NotImplementedError(\"Custom Metrics must implement this function\")\n",
    "\n",
    "    def __call__(self, y_pred: Tensor, y_true: Tensor):\n",
    "        raise NotImplementedError(\"Custom Metrics must implement this function\")\n",
    "\n",
    "\n",
    "class MultipleMetrics(object):\n",
    "    def __init__(self, metrics: List[Metric], prefix: str = \"\"):\n",
    "\n",
    "        instantiated_metrics = []\n",
    "        for metric in metrics:\n",
    "            if isinstance(metric, type):\n",
    "                instantiated_metrics.append(metric())\n",
    "            else:\n",
    "                instantiated_metrics.append(metric)\n",
    "        self._metrics = instantiated_metrics\n",
    "        self.prefix = prefix\n",
    "\n",
    "    def reset(self):\n",
    "        for metric in self._metrics:\n",
    "            metric.reset()\n",
    "\n",
    "    def __call__(self, y_pred: Tensor, y_true: Tensor) -> Dict:\n",
    "        logs = {}\n",
    "        for metric in self._metrics:\n",
    "            if isinstance(metric, Metric):\n",
    "                logs[self.prefix + metric._name] = metric(y_pred, y_true)\n",
    "            elif isinstance(metric, TorchMetric):\n",
    "                metric.update(y_pred, y_true.int())  # type: ignore[attr-defined]\n",
    "                logs[self.prefix + type(metric).__name__] = (\n",
    "                    metric.compute().detach().cpu().numpy()\n",
    "                )\n",
    "        return logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a236fc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# callbacks\n",
    "class Callback(object):\n",
    "    \"\"\"\n",
    "    Base class used to build new callbacks.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def set_params(self, params):\n",
    "        self.params = params\n",
    "\n",
    "    def set_model(self, model: Any):\n",
    "        self.model = model\n",
    "\n",
    "    def set_trainer(self, trainer: Any):\n",
    "        self.trainer = trainer\n",
    "\n",
    "    def on_epoch_begin(self, epoch: int, logs: Optional[Dict] = None):\n",
    "        pass\n",
    "\n",
    "    def on_epoch_end(\n",
    "        self, epoch: int, logs: Optional[Dict] = None, metric: Optional[float] = None\n",
    "    ):\n",
    "        pass\n",
    "\n",
    "    def on_batch_begin(self, batch: int, logs: Optional[Dict] = None):\n",
    "        pass\n",
    "\n",
    "    def on_batch_end(self, batch: int, logs: Optional[Dict] = None):\n",
    "        pass\n",
    "\n",
    "    def on_train_begin(self, logs: Optional[Dict] = None):\n",
    "        pass\n",
    "\n",
    "    def on_train_end(self, logs: Optional[Dict] = None):\n",
    "        pass\n",
    "\n",
    "    def on_eval_begin(self, logs: Optional[Dict] = None):\n",
    "        # at the moment only used to reset metrics before eval\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56f3dd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforms\n",
    "from torchvision.transforms import (\n",
    "    Pad,\n",
    "    Lambda,\n",
    "    Resize,\n",
    "    Compose,\n",
    "    TenCrop,\n",
    "    FiveCrop,\n",
    "    ToTensor,\n",
    "    Grayscale,\n",
    "    Normalize,\n",
    "    CenterCrop,\n",
    "    RandomCrop,\n",
    "    ToPILImage,\n",
    "    ColorJitter,\n",
    "    PILToTensor,\n",
    "    RandomApply,\n",
    "    RandomOrder,\n",
    "    GaussianBlur,\n",
    "    RandomAffine,\n",
    "    RandomChoice,\n",
    "    RandomInvert,\n",
    "    RandomErasing,\n",
    "    RandomEqualize,\n",
    "    RandomRotation,\n",
    "    RandomSolarize,\n",
    "    RandomGrayscale,\n",
    "    RandomPosterize,\n",
    "    ConvertImageDtype,\n",
    "    InterpolationMode,\n",
    "    RandomPerspective,\n",
    "    RandomResizedCrop,\n",
    "    RandomAutocontrast,\n",
    "    RandomVerticalFlip,\n",
    "    LinearTransformation,\n",
    "    RandomHorizontalFlip,\n",
    "    RandomAdjustSharpness,\n",
    ")\n",
    "\n",
    "Transforms = Union[\n",
    "    Pad,\n",
    "    Lambda,\n",
    "    Resize,\n",
    "    Compose,\n",
    "    TenCrop,\n",
    "    FiveCrop,\n",
    "    ToTensor,\n",
    "    Grayscale,\n",
    "    Normalize,\n",
    "    CenterCrop,\n",
    "    RandomCrop,\n",
    "    ToPILImage,\n",
    "    ColorJitter,\n",
    "    PILToTensor,\n",
    "    RandomApply,\n",
    "    RandomOrder,\n",
    "    GaussianBlur,\n",
    "    RandomAffine,\n",
    "    RandomChoice,\n",
    "    RandomInvert,\n",
    "    RandomErasing,\n",
    "    RandomEqualize,\n",
    "    RandomRotation,\n",
    "    RandomSolarize,\n",
    "    RandomGrayscale,\n",
    "    RandomPosterize,\n",
    "    ConvertImageDtype,\n",
    "    InterpolationMode,\n",
    "    RandomPerspective,\n",
    "    RandomResizedCrop,\n",
    "    RandomAutocontrast,\n",
    "    RandomVerticalFlip,\n",
    "    LinearTransformation,\n",
    "    RandomHorizontalFlip,\n",
    "    RandomAdjustSharpness,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a745b03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializer\n",
    "class Initializer(object):\n",
    "    def __call__(self, model: nn.Module):\n",
    "        raise NotImplementedError(\"Initializer must implement this method\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e75734e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseTabularModelWithAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        column_idx: Dict[str, int],\n",
    "        cat_embed_input: Optional[List[Tuple[str, int]]],\n",
    "        cat_embed_dropout: float,\n",
    "        use_cat_bias: bool,\n",
    "        cat_embed_activation: Optional[str],\n",
    "        full_embed_dropout: bool,\n",
    "        shared_embed: bool,\n",
    "        add_shared_embed: bool,\n",
    "        frac_shared_embed: float,\n",
    "        continuous_cols: Optional[List[str]],\n",
    "        cont_norm_layer: str,\n",
    "        embed_continuous: bool,\n",
    "        cont_embed_dropout: float,\n",
    "        use_cont_bias: bool,\n",
    "        cont_embed_activation: Optional[str],\n",
    "        input_dim: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.column_idx = column_idx\n",
    "        self.cat_embed_input = cat_embed_input\n",
    "        self.cat_embed_dropout = cat_embed_dropout\n",
    "        self.use_cat_bias = use_cat_bias\n",
    "        self.cat_embed_activation = cat_embed_activation\n",
    "        self.full_embed_dropout = full_embed_dropout\n",
    "        self.shared_embed = shared_embed\n",
    "        self.add_shared_embed = add_shared_embed\n",
    "        self.frac_shared_embed = frac_shared_embed\n",
    "\n",
    "        self.continuous_cols = continuous_cols\n",
    "        self.cont_norm_layer = cont_norm_layer\n",
    "        self.embed_continuous = embed_continuous\n",
    "        self.cont_embed_dropout = cont_embed_dropout\n",
    "        self.use_cont_bias = use_cont_bias\n",
    "        self.cont_embed_activation = cont_embed_activation\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "\n",
    "        self.cat_and_cont_embed = SameSizeCatAndContEmbeddings(\n",
    "            input_dim,\n",
    "            column_idx,\n",
    "            cat_embed_input,\n",
    "            cat_embed_dropout,\n",
    "            use_cat_bias,\n",
    "            full_embed_dropout,\n",
    "            shared_embed,\n",
    "            add_shared_embed,\n",
    "            frac_shared_embed,\n",
    "            continuous_cols,\n",
    "            cont_norm_layer,\n",
    "            embed_continuous,\n",
    "            cont_embed_dropout,\n",
    "            use_cont_bias,\n",
    "        )\n",
    "        self.cat_embed_act_fn = (\n",
    "            get_activation_fn(cat_embed_activation)\n",
    "            if cat_embed_activation is not None\n",
    "            else None\n",
    "        )\n",
    "        self.cont_embed_act_fn = (\n",
    "            get_activation_fn(cont_embed_activation)\n",
    "            if cont_embed_activation is not None\n",
    "            else None\n",
    "        )\n",
    "\n",
    "    def _get_embeddings(self, X: Tensor) -> Tensor:\n",
    "        x_cat, x_cont = self.cat_and_cont_embed(X)\n",
    "        if x_cat is not None:\n",
    "            x = (\n",
    "                self.cat_embed_act_fn(x_cat)\n",
    "                if self.cat_embed_act_fn is not None\n",
    "                else x_cat\n",
    "            )\n",
    "        if x_cont is not None:\n",
    "            if self.cont_embed_act_fn is not None:\n",
    "                x_cont = self.cont_embed_act_fn(x_cont)\n",
    "            x = torch.cat([x, x_cont], 1) if x_cat is not None else x_cont\n",
    "        return x\n",
    "\n",
    "    @property\n",
    "    def output_dim(self) -> int:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @property\n",
    "    def attention_weights(self):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f2793ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaintEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        n_heads: int,\n",
    "        use_bias: bool,\n",
    "        attn_dropout: float,\n",
    "        ff_dropout: float,\n",
    "        activation: str,\n",
    "        n_feat: int,\n",
    "    ):\n",
    "        super(SaintEncoder, self).__init__()\n",
    "\n",
    "        self.n_feat = n_feat\n",
    "\n",
    "        self.col_attn = MultiHeadedAttention(\n",
    "            input_dim,\n",
    "            n_heads,\n",
    "            use_bias,\n",
    "            attn_dropout,\n",
    "        )\n",
    "        self.col_attn_ff = FeedForward(input_dim, ff_dropout, activation)\n",
    "        self.col_attn_addnorm = AddNorm(input_dim, attn_dropout)\n",
    "        self.col_attn_ff_addnorm = AddNorm(input_dim, ff_dropout)\n",
    "\n",
    "        self.row_attn = MultiHeadedAttention(\n",
    "            n_feat * input_dim,\n",
    "            n_heads,\n",
    "            use_bias,\n",
    "            attn_dropout,\n",
    "        )\n",
    "        self.row_attn_ff = FeedForward(n_feat * input_dim, ff_dropout, activation)\n",
    "        self.row_attn_addnorm = AddNorm(n_feat * input_dim, attn_dropout)\n",
    "        self.row_attn_ff_addnorm = AddNorm(n_feat * input_dim, ff_dropout)\n",
    "\n",
    "    def forward(self, X: Tensor) -> Tensor:\n",
    "        x = self.col_attn_addnorm(X, self.col_attn)\n",
    "        x = self.col_attn_ff_addnorm(x, self.col_attn_ff)\n",
    "        x = einops.rearrange(x, \"b n d -> 1 b (n d)\")\n",
    "        x = self.row_attn_addnorm(x, self.row_attn)\n",
    "        x = self.row_attn_ff_addnorm(x, self.row_attn_ff)\n",
    "        x = einops.rearrange(x, \"1 b (n d) -> b n d\", n=self.n_feat)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c3071503",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        dropout: float,\n",
    "        activation: str,\n",
    "        mult: float = 4.0,\n",
    "    ):\n",
    "        super(FeedForward, self).__init__()\n",
    "        ff_hidden_dim = int(input_dim * mult)\n",
    "        self.w_1 = nn.Linear(\n",
    "            input_dim,\n",
    "            ff_hidden_dim * 2 if activation.endswith(\"glu\") else ff_hidden_dim,\n",
    "        )\n",
    "        self.w_2 = nn.Linear(ff_hidden_dim, input_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = get_activation_fn(activation)\n",
    "\n",
    "    def forward(self, X: Tensor) -> Tensor:\n",
    "        return self.w_2(self.dropout(self.activation(self.w_1(X))))\n",
    "\n",
    "    \n",
    "class AddNorm(nn.Module):\n",
    "    \"\"\"aka PosNorm\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim: int, dropout: float):\n",
    "        super(AddNorm, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.ln = nn.LayerNorm(input_dim)\n",
    "\n",
    "    def forward(self, X: Tensor, sublayer: nn.Module) -> Tensor:\n",
    "        return self.ln(X + self.dropout(sublayer(X)))\n",
    "\n",
    "    \n",
    "def get_activation_fn(activation):\n",
    "    if activation == \"relu\":\n",
    "        return nn.ReLU(inplace=True)\n",
    "    elif activation == \"leaky_relu\":\n",
    "        return nn.LeakyReLU(inplace=True)\n",
    "    elif activation == \"tanh\":\n",
    "        return nn.Tanh()\n",
    "    elif activation == \"gelu\":\n",
    "        return nn.GELU()\n",
    "    elif activation == \"geglu\":\n",
    "        return GEGLU()\n",
    "    elif activation == \"reglu\":\n",
    "        return REGLU()\n",
    "    elif activation == \"softplus\":\n",
    "        return nn.Softplus()\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"Only the following activation functions are currently \"\n",
    "            \"supported: {}. Note that 'geglu' and 'reglu' \"\n",
    "            \"should only be used as transformer's activations\".format(\n",
    "                \", \".join(allowed_activations)\n",
    "            )\n",
    "        )        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0e502d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        n_heads: int,\n",
    "        use_bias: bool,\n",
    "        dropout: float,\n",
    "        query_dim: Optional[int] = None,\n",
    "    ):\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "\n",
    "        assert input_dim % n_heads == 0, \"'input_dim' must be divisible by 'n_heads'\"\n",
    "\n",
    "        self.head_dim = input_dim // n_heads\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        query_dim = query_dim if query_dim is not None else input_dim\n",
    "        self.q_proj = nn.Linear(query_dim, input_dim, bias=use_bias)\n",
    "        self.kv_proj = nn.Linear(input_dim, input_dim * 2, bias=use_bias)\n",
    "        self.out_proj = (\n",
    "            nn.Linear(input_dim, query_dim, bias=use_bias) if n_heads > 1 else None\n",
    "        )\n",
    "\n",
    "    def forward(self, X_Q: Tensor, X_KV: Optional[Tensor] = None) -> Tensor:\n",
    "        # b: batch size\n",
    "        # s: seq length\n",
    "        # l: target sequence length\n",
    "        # m: used to refer indistinctively to s or l\n",
    "        # h: number of attention heads,\n",
    "        # d: head_dim\n",
    "        q = self.q_proj(X_Q)\n",
    "        X_KV = X_KV if X_KV is not None else X_Q\n",
    "        k, v = self.kv_proj(X_KV).chunk(2, dim=-1)\n",
    "        q, k, v = map(\n",
    "            lambda t: einops.rearrange(t, \"b m (h d) -> b h m d\", h=self.n_heads),\n",
    "            (q, k, v),\n",
    "        )\n",
    "        scores = einsum(\"b h s d, b h l d -> b h s l\", q, k) / math.sqrt(self.head_dim)\n",
    "        attn_weights = scores.softmax(dim=-1)\n",
    "        self.attn_weights = attn_weights\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        attn_output = einsum(\"b h s l, b h l d -> b h s d\", attn_weights, v)\n",
    "        output = einops.rearrange(attn_output, \"b h s d -> b s (h d)\", h=self.n_heads)\n",
    "\n",
    "        if self.out_proj is not None:\n",
    "            output = self.out_proj(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7254884f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_hidden: List[int],\n",
    "        activation: str,\n",
    "        dropout: Optional[Union[float, List[float]]],\n",
    "        batchnorm: bool,\n",
    "        batchnorm_last: bool,\n",
    "        linear_first: bool,\n",
    "    ):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        if not dropout:\n",
    "            dropout = [0.0] * len(d_hidden)\n",
    "        elif isinstance(dropout, float):\n",
    "            dropout = [dropout] * len(d_hidden)\n",
    "\n",
    "        self.mlp = nn.Sequential()\n",
    "        for i in range(1, len(d_hidden)):\n",
    "            self.mlp.add_module(\n",
    "                \"dense_layer_{}\".format(i - 1),\n",
    "                dense_layer(\n",
    "                    d_hidden[i - 1],\n",
    "                    d_hidden[i],\n",
    "                    activation,\n",
    "                    dropout[i - 1],\n",
    "                    batchnorm and (i != len(d_hidden) - 1 or batchnorm_last),\n",
    "                    linear_first,\n",
    "                ),\n",
    "            )\n",
    "\n",
    "    def forward(self, X: Tensor) -> Tensor:\n",
    "        return self.mlp(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "56758587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_layer\n",
    "class SameSizeCatAndContEmbeddings(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim: int,\n",
    "        column_idx: Dict[str, int],\n",
    "        cat_embed_input: Optional[List[Tuple[str, int]]],\n",
    "        cat_embed_dropout: float,\n",
    "        use_cat_bias: bool,\n",
    "        full_embed_dropout: bool,\n",
    "        shared_embed: bool,\n",
    "        add_shared_embed: bool,\n",
    "        frac_shared_embed: float,\n",
    "        continuous_cols: Optional[List[str]],\n",
    "        cont_norm_layer: str,\n",
    "        embed_continuous: bool,\n",
    "        cont_embed_dropout: float,\n",
    "        use_cont_bias: bool,\n",
    "    ):\n",
    "        super(SameSizeCatAndContEmbeddings, self).__init__()\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.cat_embed_input = cat_embed_input\n",
    "        self.continuous_cols = continuous_cols\n",
    "        self.embed_continuous = embed_continuous\n",
    "\n",
    "        # Categorical\n",
    "        if cat_embed_input is not None:\n",
    "            self.cat_embed = SameSizeCatEmbeddings(\n",
    "                embed_dim,\n",
    "                column_idx,\n",
    "                cat_embed_input,\n",
    "                cat_embed_dropout,\n",
    "                use_cat_bias,\n",
    "                full_embed_dropout,\n",
    "                shared_embed,\n",
    "                add_shared_embed,\n",
    "                frac_shared_embed,\n",
    "            )\n",
    "        # Continuous\n",
    "        if continuous_cols is not None:\n",
    "            self.cont_idx = [column_idx[col] for col in continuous_cols]\n",
    "            if cont_norm_layer == \"layernorm\":\n",
    "                self.cont_norm: NormLayers = nn.LayerNorm(len(continuous_cols))\n",
    "            elif cont_norm_layer == \"batchnorm\":\n",
    "                self.cont_norm = nn.BatchNorm1d(len(continuous_cols))\n",
    "            else:\n",
    "                self.cont_norm = nn.Identity()\n",
    "            if self.embed_continuous:\n",
    "                self.cont_embed = ContEmbeddings(\n",
    "                    len(continuous_cols),\n",
    "                    embed_dim,\n",
    "                    cont_embed_dropout,\n",
    "                    use_cont_bias,\n",
    "                )\n",
    "\n",
    "    def forward(self, X: Tensor) -> Tuple[Tensor, Any]:\n",
    "\n",
    "        if self.cat_embed_input is not None:\n",
    "            x_cat = self.cat_embed(X)\n",
    "        else:\n",
    "            x_cat = None\n",
    "\n",
    "        if self.continuous_cols is not None:\n",
    "            x_cont = self.cont_norm((X[:, self.cont_idx].float()))\n",
    "            if self.embed_continuous:\n",
    "                x_cont = self.cont_embed(x_cont)\n",
    "        else:\n",
    "            x_cont = None\n",
    "\n",
    "        return x_cat, x_cont\n",
    "    \n",
    "class SameSizeCatEmbeddings(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim: int,\n",
    "        column_idx: Dict[str, int],\n",
    "        embed_input: Optional[List[Tuple[str, int]]],\n",
    "        embed_dropout: float,\n",
    "        use_bias: bool,\n",
    "        full_embed_dropout: bool,\n",
    "        shared_embed: bool,\n",
    "        add_shared_embed: bool,\n",
    "        frac_shared_embed: float,\n",
    "    ):\n",
    "        super(SameSizeCatEmbeddings, self).__init__()\n",
    "\n",
    "        self.n_tokens = sum([ei[1] for ei in embed_input])\n",
    "        self.column_idx = column_idx\n",
    "        self.embed_input = embed_input\n",
    "        self.shared_embed = shared_embed\n",
    "        self.with_cls_token = \"cls_token\" in column_idx\n",
    "\n",
    "        self.embed_layers_names = None\n",
    "        if self.embed_input is not None:\n",
    "            self.embed_layers_names = {\n",
    "                e[0]: e[0].replace(\".\", \"_\") for e in self.embed_input\n",
    "            }\n",
    "\n",
    "        categorical_cols = [ei[0] for ei in embed_input]\n",
    "        self.cat_idx = [self.column_idx[col] for col in categorical_cols]\n",
    "\n",
    "        if use_bias:\n",
    "            if shared_embed:\n",
    "                warnings.warn(\n",
    "                    \"The current implementation of 'SharedEmbeddings' does not use bias\",\n",
    "                    UserWarning,\n",
    "                )\n",
    "            n_cat = (\n",
    "                len(categorical_cols) - 1\n",
    "                if self.with_cls_token\n",
    "                else len(categorical_cols)\n",
    "            )\n",
    "            self.bias = nn.init.kaiming_uniform_(\n",
    "                nn.Parameter(torch.Tensor(n_cat, embed_dim)), a=math.sqrt(5)\n",
    "            )\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "        # Categorical: val + 1 because 0 is reserved for padding/unseen cateogories.\n",
    "        if self.shared_embed:\n",
    "            self.embed: Union[nn.ModuleDict, nn.Embedding] = nn.ModuleDict(\n",
    "                {\n",
    "                    \"emb_layer_\"\n",
    "                    + self.embed_layers_names[col]: SharedEmbeddings(\n",
    "                        val if col == \"cls_token\" else val + 1,\n",
    "                        embed_dim,\n",
    "                        embed_dropout,\n",
    "                        full_embed_dropout,\n",
    "                        add_shared_embed,\n",
    "                        frac_shared_embed,\n",
    "                    )\n",
    "                    for col, val in self.embed_input\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "            n_tokens = sum([ei[1] for ei in embed_input])\n",
    "            self.embed = nn.Embedding(n_tokens + 1, embed_dim, padding_idx=0)\n",
    "            if full_embed_dropout:\n",
    "                self.dropout: DropoutLayers = FullEmbeddingDropout(embed_dropout)\n",
    "            else:\n",
    "                self.dropout = nn.Dropout(embed_dropout)\n",
    "\n",
    "    def forward(self, X: Tensor) -> Tensor:\n",
    "        if self.shared_embed:\n",
    "            cat_embed = [\n",
    "                self.embed[\"emb_layer_\" + self.embed_layers_names[col]](  # type: ignore[index]\n",
    "                    X[:, self.column_idx[col]].long()\n",
    "                ).unsqueeze(\n",
    "                    1\n",
    "                )\n",
    "                for col, _ in self.embed_input\n",
    "            ]\n",
    "            x = torch.cat(cat_embed, 1)\n",
    "        else:\n",
    "            x = self.embed(X[:, self.cat_idx].long())\n",
    "            if self.bias is not None:\n",
    "                if self.with_cls_token:\n",
    "                    # no bias to be learned for the [CLS] token\n",
    "                    bias = torch.cat(\n",
    "                        [torch.zeros(1, self.bias.shape[1], device=x.device), self.bias]\n",
    "                    )\n",
    "                else:\n",
    "                    bias = self.bias\n",
    "                x = x + bias.unsqueeze(0)\n",
    "\n",
    "            x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class ContEmbeddings(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_cont_cols: int,\n",
    "        embed_dim: int,\n",
    "        embed_dropout: float,\n",
    "        use_bias: bool,\n",
    "    ):\n",
    "        super(ContEmbeddings, self).__init__()\n",
    "\n",
    "        self.n_cont_cols = n_cont_cols\n",
    "        self.embed_dim = embed_dim\n",
    "        self.embed_dropout = embed_dropout\n",
    "        self.use_bias = use_bias\n",
    "\n",
    "        self.weight = nn.init.kaiming_uniform_(\n",
    "            nn.Parameter(torch.Tensor(n_cont_cols, embed_dim)), a=math.sqrt(5)\n",
    "        )\n",
    "\n",
    "        self.bias = (\n",
    "            nn.init.kaiming_uniform_(\n",
    "                nn.Parameter(torch.Tensor(n_cont_cols, embed_dim)), a=math.sqrt(5)\n",
    "            )\n",
    "            if use_bias\n",
    "            else None\n",
    "        )\n",
    "\n",
    "    def forward(self, X: Tensor) -> Tensor:\n",
    "        x = self.weight.unsqueeze(0) * X.unsqueeze(2)\n",
    "        if self.bias is not None:\n",
    "            x = x + self.bias.unsqueeze(0)\n",
    "        return F.dropout(x, self.embed_dropout, self.training)\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        s = \"{n_cont_cols}, {embed_dim}, embed_dropout={embed_dropout}, use_bias={use_bias}\"\n",
    "        return s.format(**self.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d6ff17cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAINT(BaseTabularModelWithAttention):\n",
    "    r\"\"\"Defines a [SAINT model](https://arxiv.org/abs/2106.01342) that\n",
    "    can be used as the `deeptabular` component of a Wide & Deep model or\n",
    "    independently by itself.\n",
    "    :information_source: **NOTE**: This is an slightly modified and enhanced\n",
    "     version of the model described in the paper,\n",
    "    Parameters\n",
    "    ----------\n",
    "    column_idx: Dict\n",
    "        Dict containing the index of the columns that will be passed through\n",
    "        the model. Required to slice the tensors. e.g.\n",
    "        _{'education': 0, 'relationship': 1, 'workclass': 2, ...}_\n",
    "    cat_embed_input: List, Optional, default = None\n",
    "        List of Tuples with the column name and number of unique values and\n",
    "        embedding dimension. e.g. _[(education, 11), ...]_\n",
    "    cat_embed_dropout: float, default = 0.1\n",
    "        Categorical embeddings dropout\n",
    "    use_cat_bias: bool, default = False,\n",
    "        Boolean indicating if bias will be used for the categorical embeddings\n",
    "    cat_embed_activation: Optional, str, default = None,\n",
    "        Activation function for the categorical embeddings, if any. _'tanh'_,\n",
    "        _'relu'_, _'leaky_relu'_ and _'gelu'_ are supported.\n",
    "    full_embed_dropout: bool, default = False\n",
    "        Boolean indicating if an entire embedding (i.e. the representation of\n",
    "        one column) will be dropped in the batch. See:\n",
    "        `pytorch_widedeep.models.transformers._layers.FullEmbeddingDropout`.\n",
    "        If `full_embed_dropout = True`, `cat_embed_dropout` is ignored.\n",
    "    shared_embed: bool, default = False\n",
    "        The idea behind `shared_embed` is described in the Appendix A in the\n",
    "        [TabTransformer paper](https://arxiv.org/abs/2012.06678): the\n",
    "        goal of having column embedding is to enable the model to distinguish\n",
    "        the classes in one column from those in the other columns. In other\n",
    "        words, the idea is to let the model learn which column is embedded\n",
    "        at the time.\n",
    "    add_shared_embed: bool, default = False\n",
    "        The two embedding sharing strategies are: 1) add the shared embeddings\n",
    "        to the column embeddings or 2) to replace the first\n",
    "        `frac_shared_embed` with the shared embeddings.\n",
    "        See `pytorch_widedeep.models.transformers._layers.SharedEmbeddings`\n",
    "    frac_shared_embed: float, default = 0.25\n",
    "        The fraction of embeddings that will be shared (if `add_shared_embed\n",
    "        = False`) by all the different categories for one particular\n",
    "        column.\n",
    "    continuous_cols: List, Optional, default = None\n",
    "        List with the name of the numeric (aka continuous) columns\n",
    "    cont_norm_layer: str, default =  \"batchnorm\"\n",
    "        Type of normalization layer applied to the continuous features. Options\n",
    "        are: _'layernorm'_, _'batchnorm'_ or None.\n",
    "    cont_embed_dropout: float, default = 0.1,\n",
    "        Continuous embeddings dropout\n",
    "    use_cont_bias: bool, default = True,\n",
    "        Boolean indicating if bias will be used for the continuous embeddings\n",
    "    cont_embed_activation: str, default = None\n",
    "        Activation function to be applied to the continuous embeddings, if\n",
    "        any. _'tanh'_, _'relu'_, _'leaky_relu'_ and _'gelu'_ are supported.\n",
    "    input_dim: int, default = 32\n",
    "        The so-called *dimension of the model*. Is the number of\n",
    "        embeddings used to encode the categorical and/or continuous columns\n",
    "    n_heads: int, default = 8\n",
    "        Number of attention heads per Transformer block\n",
    "    use_qkv_bias: bool, default = False\n",
    "        Boolean indicating whether or not to use bias in the Q, K, and V\n",
    "        projection layers\n",
    "    n_blocks: int, default = 2\n",
    "        Number of SAINT-Transformer blocks.\n",
    "    attn_dropout: float, default = 0.2\n",
    "        Dropout that will be applied to the Multi-Head Attention column and\n",
    "        row layers\n",
    "    ff_dropout: float, default = 0.1\n",
    "        Dropout that will be applied to the FeedForward network\n",
    "    transformer_activation: str, default = \"gelu\"\n",
    "        Transformer Encoder activation function. _'tanh'_, _'relu'_,\n",
    "        _'leaky_relu'_, _'gelu'_, _'geglu'_ and _'reglu'_ are supported\n",
    "    mlp_hidden_dims: List, Optional, default = None\n",
    "        MLP hidden dimensions. If not provided it will default to $[l, 4\n",
    "        \\times l, 2 \\times l]$ where $l$ is the MLP's input dimension\n",
    "    mlp_activation: str, default = \"relu\"\n",
    "        MLP activation function. _'tanh'_, _'relu'_, _'leaky_relu'_ and\n",
    "        _'gelu'_ are supported\n",
    "    mlp_dropout: float, default = 0.1\n",
    "        Dropout that will be applied to the final MLP\n",
    "    mlp_batchnorm: bool, default = False\n",
    "        Boolean indicating whether or not to apply batch normalization to the\n",
    "        dense layers\n",
    "    mlp_batchnorm_last: bool, default = False\n",
    "        Boolean indicating whether or not to apply batch normalization to the\n",
    "        last of the dense layers\n",
    "    mlp_linear_first: bool, default = False\n",
    "        Boolean indicating whether the order of the operations in the dense\n",
    "        layer. If `True: [LIN -> ACT -> BN -> DP]`. If `False: [BN -> DP ->\n",
    "        LIN -> ACT]`\n",
    "    Attributes\n",
    "    ----------\n",
    "    cat_and_cont_embed: nn.Module\n",
    "        This is the module that processes the categorical and continuous columns\n",
    "    encoder: nn.Module\n",
    "        Sequence of SAINT-Transformer blocks\n",
    "    mlp: nn.Module\n",
    "        MLP component in the model\n",
    "    Examples\n",
    "    --------\n",
    "    >>> import torch\n",
    "    >>> from pytorch_widedeep.models import SAINT\n",
    "    >>> X_tab = torch.cat((torch.empty(5, 4).random_(4), torch.rand(5, 1)), axis=1)\n",
    "    >>> colnames = ['a', 'b', 'c', 'd', 'e']\n",
    "    >>> cat_embed_input = [(u,i) for u,i in zip(colnames[:4], [4]*4)]\n",
    "    >>> continuous_cols = ['e']\n",
    "    >>> column_idx = {k:v for v,k in enumerate(colnames)}\n",
    "    >>> model = SAINT(column_idx=column_idx, cat_embed_input=cat_embed_input, continuous_cols=continuous_cols)\n",
    "    >>> out = model(X_tab)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        column_idx: Dict[str, int],\n",
    "        cat_embed_input: Optional[List[Tuple[str, int]]] = None,\n",
    "        cat_embed_dropout: float = 0.1,\n",
    "        use_cat_bias: bool = False,\n",
    "        cat_embed_activation: Optional[str] = None,\n",
    "        full_embed_dropout: bool = False,\n",
    "        shared_embed: bool = False,\n",
    "        add_shared_embed: bool = False,\n",
    "        frac_shared_embed: float = 0.25,\n",
    "        continuous_cols: Optional[List[str]] = None,\n",
    "        cont_norm_layer: str = None,\n",
    "        cont_embed_dropout: float = 0.1,\n",
    "        use_cont_bias: bool = True,\n",
    "        cont_embed_activation: Optional[str] = None,\n",
    "        input_dim: int = 32,\n",
    "        use_qkv_bias: bool = False,\n",
    "        n_heads: int = 8,\n",
    "        n_blocks: int = 2,\n",
    "        attn_dropout: float = 0.1,\n",
    "        ff_dropout: float = 0.2,\n",
    "        transformer_activation: str = \"gelu\",\n",
    "        mlp_hidden_dims: Optional[List[int]] = None,\n",
    "        mlp_activation: str = \"relu\",\n",
    "        mlp_dropout: float = 0.1,\n",
    "        mlp_batchnorm: bool = False,\n",
    "        mlp_batchnorm_last: bool = False,\n",
    "        mlp_linear_first: bool = True,\n",
    "    ):\n",
    "        super(SAINT, self).__init__(\n",
    "            column_idx=column_idx,\n",
    "            cat_embed_input=cat_embed_input,\n",
    "            cat_embed_dropout=cat_embed_dropout,\n",
    "            use_cat_bias=use_cat_bias,\n",
    "            cat_embed_activation=cat_embed_activation,\n",
    "            full_embed_dropout=full_embed_dropout,\n",
    "            shared_embed=shared_embed,\n",
    "            add_shared_embed=add_shared_embed,\n",
    "            frac_shared_embed=frac_shared_embed,\n",
    "            continuous_cols=continuous_cols,\n",
    "            cont_norm_layer=cont_norm_layer,\n",
    "            embed_continuous=True,\n",
    "            cont_embed_dropout=cont_embed_dropout,\n",
    "            use_cont_bias=use_cont_bias,\n",
    "            cont_embed_activation=cont_embed_activation,\n",
    "            input_dim=input_dim,\n",
    "        )\n",
    "\n",
    "        self.use_qkv_bias = use_qkv_bias\n",
    "        self.n_heads = n_heads\n",
    "        self.n_blocks = n_blocks\n",
    "        self.attn_dropout = attn_dropout\n",
    "        self.ff_dropout = ff_dropout\n",
    "        self.transformer_activation = transformer_activation\n",
    "\n",
    "        self.mlp_hidden_dims = mlp_hidden_dims\n",
    "        self.mlp_activation = mlp_activation\n",
    "        self.mlp_dropout = mlp_dropout\n",
    "        self.mlp_batchnorm = mlp_batchnorm\n",
    "        self.mlp_batchnorm_last = mlp_batchnorm_last\n",
    "        self.mlp_linear_first = mlp_linear_first\n",
    "\n",
    "        self.with_cls_token = \"cls_token\" in column_idx\n",
    "        self.n_cat = len(cat_embed_input) if cat_embed_input is not None else 0\n",
    "        self.n_cont = len(continuous_cols) if continuous_cols is not None else 0\n",
    "        self.n_feats = self.n_cat + self.n_cont\n",
    "\n",
    "        # Embeddings are instantiated at the base model\n",
    "        # Transformer blocks\n",
    "        self.encoder = nn.Sequential()\n",
    "        for i in range(n_blocks):\n",
    "            self.encoder.add_module(\n",
    "                \"saint_block\" + str(i),\n",
    "                SaintEncoder(\n",
    "                    input_dim,\n",
    "                    n_heads,\n",
    "                    use_qkv_bias,\n",
    "                    attn_dropout,\n",
    "                    ff_dropout,\n",
    "                    transformer_activation,\n",
    "                    self.n_feats,\n",
    "                ),\n",
    "            )\n",
    "\n",
    "        self.mlp_first_hidden_dim = (\n",
    "            self.input_dim if self.with_cls_token else (self.n_feats * self.input_dim)\n",
    "        )\n",
    "\n",
    "        if mlp_hidden_dims is not None:\n",
    "            self.mlp = MLP(\n",
    "                [self.mlp_first_hidden_dim] + mlp_hidden_dims,\n",
    "                mlp_activation,\n",
    "                mlp_dropout,\n",
    "                mlp_batchnorm,\n",
    "                mlp_batchnorm_last,\n",
    "                mlp_linear_first,\n",
    "            )\n",
    "        else:\n",
    "            self.mlp = None\n",
    "\n",
    "    def forward(self, X: Tensor) -> Tensor:\n",
    "        x = self._get_embeddings(X)\n",
    "        x = self.encoder(x)\n",
    "        if self.with_cls_token:\n",
    "            x = x[:, 0, :]\n",
    "        else:\n",
    "            x = x.flatten(1)\n",
    "        if self.mlp is not None:\n",
    "            x = self.mlp(x)\n",
    "        return x\n",
    "\n",
    "    @property\n",
    "    def output_dim(self) -> int:\n",
    "        r\"\"\"The output dimension of the model. This is a required property\n",
    "        neccesary to build the `WideDeep` class\n",
    "        \"\"\"\n",
    "        return (\n",
    "            self.mlp_hidden_dims[-1]\n",
    "            if self.mlp_hidden_dims is not None\n",
    "            else self.mlp_first_hidden_dim\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def attention_weights(self) -> List:\n",
    "        r\"\"\"List with the attention weights. Each element of the list is a tuple\n",
    "        where the first and the second elements are the column and row\n",
    "        attention weights respectively\n",
    "        The shape of the attention weights is:\n",
    "        - column attention: $(N, H, F, F)$\n",
    "        - row attention: $(1, H, N, N)$\n",
    "        where $N$ is the batch size, $H$ is the number of heads and $F$ is the\n",
    "        number of features/columns in the dataset\n",
    "        \"\"\"\n",
    "        attention_weights = []\n",
    "        for blk in self.encoder:\n",
    "            attention_weights.append(\n",
    "                (blk.col_attn.attn_weights, blk.row_attn.attn_weights)\n",
    "            )\n",
    "        return attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b7c23fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base trainer\n",
    "# source: TabTransformers\n",
    "class BaseTrainer(ABC):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: SAINT,\n",
    "        objective: str,\n",
    "        custom_loss_function: Optional[Module],\n",
    "        optimizers: Optional[Union[Optimizer, Dict[str, Optimizer]]],\n",
    "        lr_schedulers: Optional[Union[LRScheduler, Dict[str, LRScheduler]]],\n",
    "        initializers: Optional[Union[Initializer, Dict[str, Initializer]]],\n",
    "        transforms: Optional[List[Transforms]],\n",
    "        callbacks: Optional[List[Callback]],\n",
    "        metrics: Optional[Union[List[Metric], List[TorchMetric]]],\n",
    "        verbose: int,\n",
    "        seed: int,\n",
    "        **kwargs,\n",
    "    ):\n",
    "\n",
    "        self._check_inputs(\n",
    "            model, objective, optimizers, lr_schedulers, custom_loss_function\n",
    "        )\n",
    "        self.device, self.num_workers = self._set_device_and_num_workers(**kwargs)\n",
    "\n",
    "        self.early_stop = False\n",
    "        self.verbose = verbose\n",
    "        self.seed = seed\n",
    "\n",
    "        self.model = model\n",
    "        if self.model.is_tabnet:\n",
    "            self.lambda_sparse = kwargs.get(\"lambda_sparse\", 1e-3)\n",
    "            self.reducing_matrix = create_explain_matrix(self.model)\n",
    "        self.model.to(self.device)\n",
    "        self.model.wd_device = self.device\n",
    "\n",
    "        self.objective = objective\n",
    "        self.method = _ObjectiveToMethod.get(objective)\n",
    "\n",
    "        self._initialize(initializers)\n",
    "        self.loss_fn = self._set_loss_fn(objective, custom_loss_function, **kwargs)\n",
    "        self.optimizer = self._set_optimizer(optimizers)\n",
    "        self.lr_scheduler = self._set_lr_scheduler(lr_schedulers, **kwargs)\n",
    "        self.transforms = self._set_transforms(transforms)\n",
    "        self._set_callbacks_and_metrics(callbacks, metrics)\n",
    "\n",
    "    @abstractmethod\n",
    "    def fit(\n",
    "        self,\n",
    "        X_wide: Optional[np.ndarray],\n",
    "        X_tab: Optional[np.ndarray],\n",
    "        X_text: Optional[np.ndarray],\n",
    "        X_img: Optional[np.ndarray],\n",
    "        X_train: Optional[Dict[str, np.ndarray]],\n",
    "        X_val: Optional[Dict[str, np.ndarray]],\n",
    "        val_split: Optional[float],\n",
    "        target: Optional[np.ndarray],\n",
    "        n_epochs: int,\n",
    "        validation_freq: int,\n",
    "        batch_size: int,\n",
    "    ):\n",
    "        raise NotImplementedError(\"Trainer.fit method not implemented\")\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(\n",
    "        self,\n",
    "        X_wide: Optional[np.ndarray],\n",
    "        X_tab: Optional[np.ndarray],\n",
    "        X_text: Optional[np.ndarray],\n",
    "        X_img: Optional[np.ndarray],\n",
    "        X_test: Optional[Dict[str, np.ndarray]],\n",
    "        batch_size: int,\n",
    "    ) -> np.ndarray:\n",
    "        raise NotImplementedError(\"Trainer.predict method not implemented\")\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict_proba(\n",
    "        self,\n",
    "        X_wide: Optional[np.ndarray],\n",
    "        X_tab: Optional[np.ndarray],\n",
    "        X_text: Optional[np.ndarray],\n",
    "        X_img: Optional[np.ndarray],\n",
    "        X_test: Optional[Dict[str, np.ndarray]],\n",
    "        batch_size: int,\n",
    "    ) -> np.ndarray:\n",
    "        raise NotImplementedError(\"Trainer.predict_proba method not implemented\")\n",
    "\n",
    "    @abstractmethod\n",
    "    def save(\n",
    "        self,\n",
    "        path: str,\n",
    "        save_state_dict: bool,\n",
    "        model_filename: str,\n",
    "    ):\n",
    "        raise NotImplementedError(\"Trainer.save method not implemented\")\n",
    "\n",
    "    def _restore_best_weights(self):\n",
    "        already_restored = any(\n",
    "            [\n",
    "                (\n",
    "                    callback.__class__.__name__ == \"EarlyStopping\"\n",
    "                    and callback.restore_best_weights\n",
    "                )\n",
    "                for callback in self.callback_container.callbacks\n",
    "            ]\n",
    "        )\n",
    "        if already_restored:\n",
    "            pass\n",
    "        else:\n",
    "            for callback in self.callback_container.callbacks:\n",
    "                if callback.__class__.__name__ == \"ModelCheckpoint\":\n",
    "                    if callback.save_best_only:\n",
    "                        if self.verbose:\n",
    "                            print(\n",
    "                                f\"Model weights restored to best epoch: {callback.best_epoch + 1}\"\n",
    "                            )\n",
    "                        self.model.load_state_dict(callback.best_state_dict)\n",
    "                    else:\n",
    "                        if self.verbose:\n",
    "                            print(\n",
    "                                \"Model weights after training corresponds to the those of the \"\n",
    "                                \"final epoch which might not be the best performing weights. Use \"\n",
    "                                \"the 'ModelCheckpoint' Callback to restore the best epoch weights.\"\n",
    "                            )\n",
    "\n",
    "    def _initialize(self, initializers):\n",
    "        if initializers is not None:\n",
    "            if isinstance(initializers, Dict):\n",
    "                self.initializer = MultipleInitializer(\n",
    "                    initializers, verbose=self.verbose\n",
    "                )\n",
    "                self.initializer.apply(self.model)\n",
    "            elif isinstance(initializers, type):\n",
    "                self.initializer = initializers()\n",
    "                self.initializer(self.model)\n",
    "            elif isinstance(initializers, Initializer):\n",
    "                self.initializer = initializers\n",
    "                self.initializer(self.model)\n",
    "\n",
    "    def _set_loss_fn(self, objective, custom_loss_function, **kwargs):\n",
    "\n",
    "        class_weight = (\n",
    "            torch.tensor(kwargs[\"class_weight\"]).to(self.device)\n",
    "            if \"class_weight\" in kwargs\n",
    "            else None\n",
    "        )\n",
    "\n",
    "        if custom_loss_function is not None:\n",
    "            return custom_loss_function\n",
    "        elif (\n",
    "            self.method not in [\"regression\", \"qregression\"]\n",
    "            and \"focal_loss\" not in objective\n",
    "        ):\n",
    "            return alias_to_loss(objective, weight=class_weight)\n",
    "        elif \"focal_loss\" in objective:\n",
    "            alpha = kwargs.get(\"alpha\", 0.25)\n",
    "            gamma = kwargs.get(\"gamma\", 2.0)\n",
    "            return alias_to_loss(objective, alpha=alpha, gamma=gamma)\n",
    "        else:\n",
    "            return alias_to_loss(objective)\n",
    "\n",
    "    def _set_optimizer(self, optimizers):\n",
    "        if optimizers is not None:\n",
    "            if isinstance(optimizers, Optimizer):\n",
    "                optimizer: Union[Optimizer, MultipleOptimizer] = optimizers\n",
    "            elif isinstance(optimizers, Dict):\n",
    "                opt_names = list(optimizers.keys())\n",
    "                mod_names = [n for n, c in self.model.named_children()]\n",
    "                # if with_fds - the prediction layer is part of the model and\n",
    "                # should be optimized with the rest of deeptabular\n",
    "                # component/model\n",
    "                if self.model.with_fds:\n",
    "                    if \"enf_pos\" in mod_names:\n",
    "                        mod_names.remove(\"enf_pos\")\n",
    "                    mod_names.remove(\"fds_layer\")\n",
    "                    optimizers[\"deeptabular\"].add_param_group(\n",
    "                        {\"params\": self.model.fds_layer.pred_layer.parameters()}\n",
    "                    )\n",
    "                for mn in mod_names:\n",
    "                    assert mn in opt_names, \"No optimizer found for {}\".format(mn)\n",
    "                optimizer = MultipleOptimizer(optimizers)\n",
    "        else:\n",
    "            optimizer = torch.optim.Adam(self.model.parameters())  # type: ignore\n",
    "        return optimizer\n",
    "\n",
    "    def _set_lr_scheduler(self, lr_schedulers, **kwargs):\n",
    "\n",
    "        # ReduceLROnPlateau is special\n",
    "        reducelronplateau_criterion = kwargs.get(\"reducelronplateau_criterion\", None)\n",
    "\n",
    "        self._set_reduce_on_plateau_criterion(\n",
    "            lr_schedulers, reducelronplateau_criterion\n",
    "        )\n",
    "\n",
    "        if lr_schedulers is not None:\n",
    "\n",
    "            if isinstance(lr_schedulers, LRScheduler) or isinstance(\n",
    "                lr_schedulers, ReduceLROnPlateau\n",
    "            ):\n",
    "                lr_scheduler = lr_schedulers\n",
    "                cyclic_lr = \"cycl\" in lr_scheduler.__class__.__name__.lower()\n",
    "            else:\n",
    "                lr_scheduler = MultipleLRScheduler(lr_schedulers)\n",
    "                scheduler_names = [\n",
    "                    sc.__class__.__name__.lower()\n",
    "                    for _, sc in lr_scheduler._schedulers.items()\n",
    "                ]\n",
    "                cyclic_lr = any([\"cycl\" in sn for sn in scheduler_names])\n",
    "        else:\n",
    "            lr_scheduler, cyclic_lr = None, False\n",
    "\n",
    "        self.cyclic_lr = cyclic_lr\n",
    "\n",
    "        return lr_scheduler\n",
    "\n",
    "    def _set_reduce_on_plateau_criterion(\n",
    "        self, lr_schedulers, reducelronplateau_criterion\n",
    "    ):\n",
    "\n",
    "        self.reducelronplateau = False\n",
    "\n",
    "        if isinstance(lr_schedulers, Dict):\n",
    "            for _, scheduler in lr_schedulers.items():\n",
    "                if isinstance(scheduler, ReduceLROnPlateau):\n",
    "                    self.reducelronplateau = True\n",
    "        elif isinstance(lr_schedulers, ReduceLROnPlateau):\n",
    "            self.reducelronplateau = True\n",
    "\n",
    "        if self.reducelronplateau and not reducelronplateau_criterion:\n",
    "            UserWarning(\n",
    "                \"The learning rate scheduler of at least one of the model components is of type \"\n",
    "                \"ReduceLROnPlateau. The step method in this scheduler requires a 'metrics' param \"\n",
    "                \"that can be either the validation loss or the validation metric. Please, when \"\n",
    "                \"instantiating the Trainer, specify which quantity will be tracked using \"\n",
    "                \"reducelronplateau_criterion = 'loss' (default) or reducelronplateau_criterion = 'metric'\"\n",
    "            )\n",
    "            self.reducelronplateau_criterion = \"loss\"\n",
    "        else:\n",
    "            self.reducelronplateau_criterion = reducelronplateau_criterion\n",
    "\n",
    "    @staticmethod\n",
    "    def _set_transforms(transforms):\n",
    "        if transforms is not None:\n",
    "            return MultipleTransforms(transforms)()\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def _set_callbacks_and_metrics(self, callbacks, metrics):\n",
    "        self.callbacks: List = [History(), LRShedulerCallback()]\n",
    "        if callbacks is not None:\n",
    "            for callback in callbacks:\n",
    "                if isinstance(callback, type):\n",
    "                    callback = callback()\n",
    "                self.callbacks.append(callback)\n",
    "        if metrics is not None:\n",
    "            self.metric = MultipleMetrics(metrics)\n",
    "            self.callbacks += [MetricCallback(self.metric)]\n",
    "        else:\n",
    "            self.metric = None\n",
    "        self.callback_container = CallbackContainer(self.callbacks)\n",
    "        self.callback_container.set_model(self.model)\n",
    "        self.callback_container.set_trainer(self)\n",
    "\n",
    "    @staticmethod\n",
    "    def _check_inputs(\n",
    "        model,\n",
    "        objective,\n",
    "        optimizers,\n",
    "        lr_schedulers,\n",
    "        custom_loss_function,\n",
    "    ):\n",
    "\n",
    "        if model.with_fds and _ObjectiveToMethod.get(objective) != \"regression\":\n",
    "            raise ValueError(\n",
    "                \"Feature Distribution Smooting can be used only for regression\"\n",
    "            )\n",
    "\n",
    "        if _ObjectiveToMethod.get(objective) == \"multiclass\" and model.pred_dim == 1:\n",
    "            raise ValueError(\n",
    "                \"This is a multiclass classification problem but the size of the output layer\"\n",
    "                \" is set to 1. Please, set the 'pred_dim' param equal to the number of classes \"\n",
    "                \" when instantiating the 'WideDeep' class\"\n",
    "            )\n",
    "\n",
    "        if isinstance(optimizers, Dict):\n",
    "            if lr_schedulers is not None and not isinstance(lr_schedulers, Dict):\n",
    "                raise ValueError(\n",
    "                    \"''optimizers' and 'lr_schedulers' must have consistent type: \"\n",
    "                    \"(Optimizer and LRScheduler) or (Dict[str, Optimizer] and Dict[str, LRScheduler]) \"\n",
    "                    \"Please, read the documentation or see the examples for more details\"\n",
    "                )\n",
    "\n",
    "        if custom_loss_function is not None and objective not in [\n",
    "            \"binary\",\n",
    "            \"multiclass\",\n",
    "            \"regression\",\n",
    "        ]:\n",
    "            raise ValueError(\n",
    "                \"If 'custom_loss_function' is not None, 'objective' must be 'binary' \"\n",
    "                \"'multiclass' or 'regression', consistent with the loss function\"\n",
    "            )\n",
    "\n",
    "    @staticmethod\n",
    "    def _set_device_and_num_workers(**kwargs):\n",
    "\n",
    "        # Important note for Mac users: Since python 3.8, the multiprocessing\n",
    "        # library start method changed from 'fork' to 'spawn'. This affects the\n",
    "        # data-loaders, which will not run in parallel.\n",
    "        default_num_workers = (\n",
    "            0\n",
    "            if sys.platform == \"darwin\" and sys.version_info.minor > 7\n",
    "            else os.cpu_count()\n",
    "        )\n",
    "        default_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        device = kwargs.get(\"device\", default_device)\n",
    "        num_workers = kwargs.get(\"num_workers\", default_num_workers)\n",
    "        return device, num_workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a4a9f063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_tab:\n",
      " tensor([[2.0000, 1.0000, 3.0000, 1.0000, 0.0404],\n",
      "        [1.0000, 2.0000, 0.0000, 2.0000, 0.8259],\n",
      "        [3.0000, 0.0000, 2.0000, 0.0000, 0.7349],\n",
      "        [2.0000, 1.0000, 1.0000, 3.0000, 0.2618],\n",
      "        [2.0000, 2.0000, 1.0000, 1.0000, 0.3920]])\n",
      "colnames:\n",
      " ['a', 'b', 'c', 'd', 'e']\n",
      "cat_embed_input:\n",
      " [('a', 4), ('b', 4), ('c', 4), ('d', 4)]\n",
      "continuous_cols:\n",
      " ['e']\n",
      "column_idx:\n",
      " {'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': 4}\n",
      "output shape:\n",
      " torch.Size([5, 160])\n"
     ]
    }
   ],
   "source": [
    "X_tab = torch.cat((torch.empty(5, 4).random_(4), torch.rand(5, 1)), axis=1)\n",
    "colnames = ['a', 'b', 'c', 'd', 'e']\n",
    "cat_embed_input = [(u,i) for u,i in zip(colnames[:4], [4]*4)]\n",
    "continuous_cols = ['e']\n",
    "column_idx = {k:v for v,k in enumerate(colnames)}\n",
    "model = SAINT(column_idx=column_idx, cat_embed_input=cat_embed_input, continuous_cols=continuous_cols)\n",
    "out = model(X_tab)\n",
    "\n",
    "print(\"X_tab:\\n\", X_tab)\n",
    "print(\"colnames:\\n\", colnames)\n",
    "print(\"cat_embed_input:\\n\", cat_embed_input)\n",
    "print(\"continuous_cols:\\n\", continuous_cols)\n",
    "print(\"column_idx:\\n\", column_idx)\n",
    "print(\"output shape:\\n\", out.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
