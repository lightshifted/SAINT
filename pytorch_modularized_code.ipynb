{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3731ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "from einops import rearrange\n",
    "from typing import List, Dict, Union\n",
    "from argparse import Namespace\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import einsum\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "from operations.data import generate_dataset\n",
    "from operations.data import generate_dataloader\n",
    "from operations.embeds import Embedding\n",
    "from operations.model import NewGELU\n",
    "from operations.utils import generate_splits\n",
    "from operations.utils import preprocess\n",
    "from operations.utils import CutMix, Mixup\n",
    "\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46338499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary for configuration settings\n",
    "config = Namespace()\n",
    "\n",
    "# where to store our train/val/test sets\n",
    "config.train_csv_path = 'data/train/target/train_targets.csv'\n",
    "config.train_y_csv_path = 'data/train/label/train_labels.csv'\n",
    "\n",
    "config.val_csv_path = 'data/val/target/val_targets.csv'\n",
    "config.val_y_csv_path = 'data/val/label/val_labels.csv'\n",
    "\n",
    "config.test_csv_path = 'data/test/target/test_targets.csv'\n",
    "config.test_y_csv_path = 'data/test/label/test_labels.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2353d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data\n",
    "data = pd.read_csv('data/creditcard.csv')\n",
    "\n",
    "# generate split indices\n",
    "sup_train_indices, val_indices, test_indices, ssl_train_indices = generate_splits(data.shape[0])\n",
    "\n",
    "# preprocess data\n",
    "df_proc, y_proc, no_num, no_cat, cats = preprocess(data.drop(columns=['Class']), data.Class, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "105571f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate train/val/test sets\n",
    "train_df, train_y = df_proc.iloc[sup_train_indices], y_proc.iloc[sup_train_indices]\n",
    "val_df, val_y = df_proc.iloc[val_indices], y_proc.iloc[val_indices]\n",
    "test_df, test_y = df_proc.iloc[test_indices], y_proc.iloc[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7440d698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader reads in files using their designated paths\n",
    "train_dataset, val_dataset, test_dataset = generate_dataset(\n",
    "                                            train_csv_path = config.train_csv_path,\n",
    "                                            val_csv_path = config.val_csv_path,\n",
    "                                            test_csv_path = config.test_csv_path,\n",
    "                                            train_y_csv_path = config.train_y_csv_path,\n",
    "                                            val_y_csv_path = config.val_y_csv_path,\n",
    "                                            test_y_csv_path = config.test_y_csv_path)\n",
    "\n",
    "\n",
    "# prepare our train, validation, and test loaders\n",
    "train_loader, validation_loader, test_loader = generate_dataloader(train_bs=16, \n",
    "                                                                   val_bs=16, \n",
    "                                                                   num_workers=0, \n",
    "                                                                   data_paths=vars(config),\n",
    "                                                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2b7a36d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.n_embd = 10\n",
    "config.no_num = no_num\n",
    "config.no_cat = no_cat\n",
    "config.cats = cats\n",
    "config.n_head = 2\n",
    "config.resid_pdrop = 0.8\n",
    "config.prob_cutmix = 0.3 # used in paper\n",
    "config.mixup_alpha = 0.2 # used in paper\n",
    "config.d_k = config.n_embd // config.n_head\n",
    "config.scale = config.n_head ** -0.5\n",
    "config.d_v = 10\n",
    "config.dim_head = 16\n",
    "config.inner_dim = config.n_head * config.dim_head\n",
    "config.d_model = no_num + no_cat\n",
    "config.mask = None\n",
    "config.alpha = 1.0\n",
    "config.attn_pdrop = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a7a3f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(train_loader)) # (16, 31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "628d918b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Xi_Pi(nn.Module):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.cut_mix = CutMix(config)\n",
    "        self.mix_up = Mixup(config)\n",
    "        \n",
    "        self.em_1 = Embedding(config.n_embd, config.no_num, config.no_cat, config.cats)\n",
    "        self.em_2 = Embedding(config.n_embd, config.no_num, config.no_cat, config.cats)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # embed batch\n",
    "        pi = self.em_1(x)\n",
    "        # embed cutmixed batch\n",
    "        pi_prime_em = self.em_2(self.cut_mix(x))\n",
    "        # mixup embedded cutmixed batch\n",
    "        pi_prime = self.mix_up(pi_prime_em)\n",
    "        \n",
    "        return pi, pi_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "01249746",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        # linear projections of the queries, keys and values h times to d_k, d_k, and d_v respectively\n",
    "        self.to_qkv = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        self.w_qs = nn.Linear(config.n_embd, config.n_head * config.d_k)\n",
    "        self.w_ks = nn.Linear(config.n_embd, config.n_head * config.d_k)\n",
    "        self.w_vs = nn.Linear(config.n_embd, config.n_head * config.n_embd)\n",
    "        \n",
    "        # initialize weights with values drawn from the normal distribution\n",
    "        nn.init.normal_(self.to_qkv.weight, mean=0, std=np.sqrt(2.0 / (config.d_model + config.n_embd)))\n",
    "        nn.init.normal_(self.w_qs.weight, mean=0, std=np.sqrt(2.0 / (config.d_model + config.d_k)))\n",
    "        nn.init.normal_(self.w_qs.weight, mean=0, std=np.sqrt(2.0 / (config.d_model + config.d_k)))\n",
    "        nn.init.normal_(self.w_qs.weight, mean=0, std=np.sqrt(2.0 / (config.d_model + config.n_embd)))\n",
    "        \n",
    "        # linear projection after attention computation\n",
    "        self.fc = nn.Linear(config.n_head * config.n_embd, config.n_embd) \n",
    "        # Xavier initialization for fc layer\n",
    "        nn.init.xavier_normal_(self.fc.weight)\n",
    "        \n",
    "        # regularization\n",
    "        self.layer_norm = nn.LayerNorm(config.n_embd)\n",
    "        self.dropout = nn.Dropout(p=config.attn_pdrop)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        # query, key, values for all heads in a batch\n",
    "        q, k, v = self.to_qkv(x).chunk(3, dim=-1) # (B, T, C)\n",
    "        \n",
    "        # residual connection\n",
    "        residual = q\n",
    "        q = rearrange(self.w_qs(q), 'b l (head k) -> head b l k', head=config.n_head) # (hs, B, T, C//hs)\n",
    "        k = rearrange(self.w_ks(k), 'b l (head k) -> head b l k', head=config.n_head) # (hs, B, T, C//hs)\n",
    "        v = rearrange(self.w_vs(v), 'b l (head k) -> head b l k', head=config.n_head) # (hs, B, T, C//hs) \n",
    "\n",
    "        # compute attention \n",
    "        attn = torch.einsum('h b l k, h b t k -> h b l t', [q, k]) / np.sqrt(q.shape[-1]) # (hs, B, T, T)\n",
    "        if mask is not None:\n",
    "            attn = attn.masked_fill(mask[None], -np.inf)    \n",
    "        attn = torch.softmax(attn, dim=3)  # (hs, B, T, T)\n",
    "\n",
    "        # compute output\n",
    "        output = torch.einsum('h b l t, h b t v -> h b l v', [attn, v]) # (hs, B, T, C)\n",
    "        output = rearrange(output, 'head b l v -> b l (head v)') # (B, T, C)\n",
    "\n",
    "        # apply dropout to linearly projected output\n",
    "        output = self.dropout(self.fc(output))\n",
    "        # apply layer normalization to sum of output and residual connection\n",
    "        output = self.layer_norm(output + residual)\n",
    "        return output, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "5c28ec72",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntersampleAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        # linear projections of the queries, keys and values h times to d_k, d_k, and d_v respectively\n",
    "        self.to_qkv = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        self.w_qs = nn.Linear(config.n_embd, config.n_head * config.d_k)\n",
    "        self.w_ks = nn.Linear(config.n_embd, config.n_head * config.d_k)\n",
    "        self.w_vs = nn.Linear(config.n_embd, config.n_head * config.n_embd)\n",
    "        \n",
    "        # initialize weights with values drawn from the normal distribution\n",
    "        nn.init.normal_(self.to_qkv.weight, mean=0, std=np.sqrt(2.0 / (config.d_model + config.n_embd)))\n",
    "        nn.init.normal_(self.w_qs.weight, mean=0, std=np.sqrt(2.0 / (config.d_model + config.d_k)))\n",
    "        nn.init.normal_(self.w_qs.weight, mean=0, std=np.sqrt(2.0 / (config.d_model + config.d_k)))\n",
    "        nn.init.normal_(self.w_qs.weight, mean=0, std=np.sqrt(2.0 / (config.d_model + config.n_embd)))\n",
    "        \n",
    "        # linear projection after attention computation\n",
    "        self.fc = nn.Linear(config.n_embd, config.n_embd) \n",
    "        # Xavier initialization for fc layer\n",
    "        nn.init.xavier_normal_(self.fc.weight)\n",
    "        \n",
    "        # regularization\n",
    "        self.layer_norm = nn.LayerNorm(config.n_embd)\n",
    "        self.dropout = nn.Dropout(p=config.attn_pdrop)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        q, k, v = self.to_qkv(pi_attended).chunk(3, dim=-1)\n",
    "        \n",
    "        # residual connection\n",
    "        residual = q\n",
    "        q = rearrange(w_qs(q), 'b w (d h) -> () b h (w d)', h=config.n_head) # (1, B, h, (T * C) // 2) \n",
    "        k = rearrange(w_ks(k), 'b w (d h) -> () b h (w d)', h=config.n_head) # (1, B, h, (T * C) // 2) \n",
    "        v = rearrange(w_vs(v), 'b w (d h) -> () b h (w d)', h=config.n_head) # (1, B, h, (T * C) // 2) \n",
    "\n",
    "        # compute attention\n",
    "        attn = torch.einsum('h b l k, h b t k -> h b l t', [q, k]) / np.sqrt(q.shape[-1]) # (1, B, h, h)\n",
    "        attn = torch.softmax(attn, dim=3) # (1, B, h, h)\n",
    "        \n",
    "        # compute output\n",
    "        output = torch.einsum('h b l t, h b t v -> h b l v', [attn, v]) # (1, B, h, T * C)\n",
    "        output = output.view(16, 31, 10)\n",
    "        \n",
    "        output = self.dropout(self.fc(output))\n",
    "        output = self.layer_norm(output + residual)\n",
    "        return output, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "88064356",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.feed_forward = nn.ModuleDict(dict(\n",
    "            proj_1 = nn.Linear(config.n_embd, 20),\n",
    "            proj_2 = nn.Linear(20, config.n_embd),\n",
    "            dropout = nn.Dropout(0.1),\n",
    "            activation = NewGELU()\n",
    "            ))\n",
    "\n",
    "        m = self.feed_forward\n",
    "\n",
    "        self.mlpf = lambda x: m.proj_2(m.dropout(m.activation(m.proj_1(x))))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.mlpf(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "16eb6f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaintPipeline(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layer_norm = nn.LayerNorm(config.n_embd)\n",
    "        self.multihead_attention = MultiHeadAttention(config)\n",
    "        self.FF1 = FeedForward(config)\n",
    "        self.MISA = IntersampleAttention(config)\n",
    "        self.FF2 = FeedForward(config)\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        # compute multi-head attention\n",
    "        z1 = self.layer_norm(x_attn) + x_attn\n",
    "        z2 = self.layer_norm(self.FF1(z1)) + z1\n",
    "        z2_attn, z2_attn_mask = self.MISA(z2)\n",
    "        z3 = self.layer_norm(z2_attn) + z2\n",
    "        r = self.layer_norm(self.FF2(z3)) + z3\n",
    "        return r\n",
    "    \n",
    "sp = SaintPipeline(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e138bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
