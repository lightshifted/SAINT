# Self-Attention and Intersample Attention Transformer (SAINT) - PyTorch
## Contrastive Learning for Tabular Data

Paper: [https://arxiv.org/pdf/2106.01342.pdf](https://arxiv.org/pdf/2106.01342.pdf)

Implementation of Self-Attention Transformer, a simple way to achieve SOTA in classification with tabular data (even beats XGBoost!). The core model architecture is motivated by the Vaswani, et. al. is seminal paper [Attention is All You Need](https://arxiv.org/abs/1706.03762).