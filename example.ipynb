{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d37ea3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "from einops import rearrange\n",
    "from typing import List, Dict, Union\n",
    "from argparse import Namespace\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import einsum\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "from operations.data import generate_dataset\n",
    "from operations.data import generate_dataloader\n",
    "from operations.embeds import Embedding\n",
    "from operations.model import NewGELU\n",
    "from operations.utils import generate_splits\n",
    "from operations.utils import preprocess\n",
    "from operations.utils import CutMix, Mixup\n",
    "\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64e6ed22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data\n",
    "data = pd.read_csv('data/creditcard.csv')\n",
    "\n",
    "# generate split indices\n",
    "sup_train_indices, val_indices, test_indices, ssl_train_indices = generate_splits(data.shape[0])\n",
    "\n",
    "# preprocess data\n",
    "df_proc, y_proc, no_num, no_cat, cats = preprocess(data.drop(columns=['Class']), data.Class, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe0ecb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate train/val/test sets\n",
    "train_df, train_y = df_proc.iloc[sup_train_indices], y_proc.iloc[sup_train_indices]\n",
    "val_df, val_y = df_proc.iloc[val_indices], y_proc.iloc[val_indices]\n",
    "test_df, test_y = df_proc.iloc[test_indices], y_proc.iloc[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c47f087",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Namespace()\n",
    "\n",
    "# where to store our train/val/test sets\n",
    "config.train_csv_path = 'data/train/target/train_targets.csv'\n",
    "config.val_csv_path = 'data/val/target/val_targets.csv'\n",
    "config.test_csv_path = 'data/test/target/test_targets.csv'\n",
    "config.train_y_csv = 'data/train/label/train_labels.csv'\n",
    "config.val_y_csv = 'data/val/label/val_labels.csv'\n",
    "config.test_y_csv = 'data/test/label/test_labels.csv'\n",
    "\n",
    "# save the preprocessed data\n",
    "train_df.to_csv(config.train_csv_path, index=False)\n",
    "train_y.to_csv(config.train_y_csv, index=False)\n",
    "\n",
    "val_df.to_csv(config.val_csv_path, index=False)\n",
    "val_y.to_csv(config.val_y_csv, index=False)\n",
    "\n",
    "test_df.to_csv(config.test_csv_path, index=False)\n",
    "test_y.to_csv(config.test_y_csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c252b389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader reads in files using their designated paths\n",
    "train_dataset, val_dataset, test_dataset = generate_dataset(\n",
    "                                            train_csv_path = config.train_csv_path,\n",
    "                                            val_csv_path = config.val_csv_path,\n",
    "                                            test_csv_path = config.test_csv_path,\n",
    "                                            train_y_csv_path = config.train_y_csv,\n",
    "                                            val_y_csv_path = config.val_y_csv,\n",
    "                                            test_y_csv_path = config.test_y_csv)\n",
    "\n",
    "# dictionary containing data paths that will be passed to the generate_dataloader class\n",
    "data_paths = {\n",
    "    \"train_csv_path\": config.train_csv_path,\n",
    "    \"val_csv_path\": config.val_csv_path,\n",
    "    \"test_csv_path\": config.test_csv_path,\n",
    "    \"train_y_csv_path\": config.train_y_csv,\n",
    "    \"val_y_csv_path\": config.val_y_csv,\n",
    "    \"test_y_csv_path\": config.test_y_csv\n",
    "}\n",
    "\n",
    "# prepare our train, validation, and test loaders\n",
    "train_loader, validation_loader, test_loader = generate_dataloader(train_bs=16, \n",
    "                                                                   val_bs=16, \n",
    "                                                                   num_workers=0, \n",
    "                                                                   data_paths=data_paths,\n",
    "                                                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3161de72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial configuration\n",
    "config.n_embd = 10\n",
    "config.no_num = no_num\n",
    "config.no_cat = no_cat\n",
    "config.cats = cats\n",
    "config.n_head = 2\n",
    "config.resid_pdrop = 0.8\n",
    "config.prob_cutmix = 0.3 # used in paper\n",
    "config.mixup_alpha = 0.2 # used in paper\n",
    "config.d_k = config.n_embd // config.n_head\n",
    "config.scale = config.n_head ** -0.5\n",
    "config.d_v = 10\n",
    "config.dim_head = 16\n",
    "config.inner_dim = config.n_head * config.dim_head\n",
    "config.block_size = no_num + no_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67383923",
   "metadata": {},
   "source": [
    "## Self Supervised Pre-Training\n",
    "<p align=\"center\">\n",
    "    <img width=\"500\" height=\"350\" src=\"media/media.jpg\">\n",
    "</p>\n",
    "\n",
    "SAINT implements contrastive pre-training, where the distance between two views of the same point is minimized while maximizing the distance between two different points. This strategy is coupled with denoising to perform pre-training on datasets with varied volumes of labeled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e317f76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = next(iter(train_loader))[0] # (16, 31)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6ff27f",
   "metadata": {},
   "source": [
    "The CutMix regularization strategy is used to augment samples in the input space, and mixup for samples in the embedding space. Specifically, mixup generates convex combinations of pairs of examples and their labels to regularize the NN to favor simple linear behaviour in-between training examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4e24fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_mix = CutMix(config.prob_cutmix)\n",
    "mix_up = Mixup(config.mixup_alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1f0393",
   "metadata": {},
   "source": [
    "Continous and categorical features are projected into the higher dimensional embedding space before being passed through the transformer blocks. A seperate single fully-connected layer with a ReLU nonlinearity is used for each continous feature to project the 1-dimensional input into d-dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5266655",
   "metadata": {},
   "outputs": [],
   "source": [
    "em_1 = Embedding(config.n_embd, config.no_num , config.no_cat, config.cats) # +1 to account for addition of <cls> token\n",
    "em_2 = Embedding(config.n_embd, config.no_num , config.no_cat, config.cats)\n",
    "\n",
    "# embed batch\n",
    "pi = em_1(x)\n",
    "# embed cutmixed batch\n",
    "pi_prime_em = em_2(cut_mix(x))\n",
    "# mixup embedded cutmixed batch\n",
    "pi_prime = mix_up(pi_prime_em)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e773614",
   "metadata": {},
   "source": [
    "## SAINT Architecture\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img width=\"255\" height=\"200\" src=\"media/saint_block.jpg\">\n",
    "</p>\n",
    "Each layer has two attention blocks: one self-attention block, and one intersample attention block. The former is identical to the transformer block proposed by Vaswani et al., where the model takes in a sequence of feature embeddings and ouputs contextual representations of the same dimension. The latter uses intersample attention in lieu of self-attention, that being the only difference in architecture between the two blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75b1d58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self-Attention block\n",
    "def self_attention(x, config, mask=False):\n",
    "    \n",
    "    # query, key, and value projections for all heads, but in a batch\n",
    "    to_qkv = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "    \n",
    "    # output projection\n",
    "    c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "    \n",
    "    # regularization\n",
    "    attn_dropout = nn.Dropout(0.1)\n",
    "    resid_dropout = nn.Dropout(0.1)\n",
    "    \n",
    "    # causal mask\n",
    "    if mask == True:\n",
    "        nn.Module.register_buffer = (\"bias\", torch.tril(\n",
    "            torch.ones(config.block_size, config.block_size)).view(\n",
    "        1, 1, config.block_size, config.block_size))\n",
    "    \n",
    "    # num heads\n",
    "    h = config.n_head\n",
    "        \n",
    "    # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "    q, k, v = to_qkv(pi).chunk(3, dim=-1)\n",
    "    q, k, v = map(lambda x: rearrange(x, 'b n (h d) -> b h n d', h=h), (q, k, v))\n",
    "\n",
    "    att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "    attn = F.softmax(att, dim=-1)\n",
    "    attn = attn_dropout(attn)\n",
    "    \n",
    "    y = att @ v\n",
    "    y = y.transpose(1, 2).contiguous().view(16, 31, 10)\n",
    "    \n",
    "    # output projection\n",
    "    y = resid_dropout(c_proj(y))\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce27018",
   "metadata": {},
   "source": [
    "Intersample attention computes attention over samples rather than features.\n",
    "<p align=\"center\">\n",
    "    <img width=\"1000\" height=\"1000\" src=\"media/intersample_attention.jpg\">\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87f458cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intersample Attention block\n",
    "def intersample_attention(x, config):\n",
    "    x = rearrange(x, 'b w d -> () b (w d)')\n",
    "    return self_attention(x, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f38f994f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed Forward\n",
    "def feed_forward(x, config):\n",
    "    proj_1 = nn.Linear(config.n_embd, 20)\n",
    "    proj_2 = nn.Linear(20, config.n_embd)\n",
    "    dropout = nn.Dropout(0.1)\n",
    "    activation = NewGELU()\n",
    "    return proj_2(dropout(activation(proj_1(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d9c76bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saint Block\n",
    "def saint_block(x, config, n_layers=1):\n",
    "    \"SAINT pipeline\"\n",
    "    while n_layers:\n",
    "\n",
    "        LN = nn.LayerNorm(config.n_embd, config.n_embd)\n",
    "        MSA = self_attention(x, config)\n",
    "\n",
    "        z1 = LN(MSA) + pi_prime\n",
    "        z2 = LN(feed_forward(z1, config)) + z1\n",
    "        z3 = LN(intersample_attention(z2, config)) + z2\n",
    "        r = LN(feed_forward(z3, config)) + z3\n",
    "        \n",
    "        n_layers -= 1\n",
    "    \n",
    "    return r # contextual representation output corresponding to x\n",
    "\n",
    "ri = saint_block(pi, config)\n",
    "ri_prime = saint_block(pi_prime, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d6c617",
   "metadata": {},
   "source": [
    "## Projection Heads\n",
    "Outputs are passed through two pojection heads, each consisting of an MLP with one hidden layer and a ReLU. The projection heads are in this case used to reduce dimensionality before computing contrastive loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf2d0f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = nn.ModuleDict(dict(\n",
    "        proj = nn.Linear(310, config.d_k),\n",
    "        activation = nn.ReLU(),\n",
    "        dropout = nn.Dropout(0.1)))\n",
    "\n",
    "mlp2 = nn.ModuleDict(dict(\n",
    "        proj = nn.Linear(310, config.d_k),\n",
    "        activation = nn.ReLU(),\n",
    "        dropout = nn.Dropout(0.1)))\n",
    "\n",
    "mlpf1 = lambda x: mlp.dropout(mlp.activation(mlp.proj(x)))\n",
    "mlpf2 = lambda x: mlp2.dropout(mlp2.activation(mlp2.proj(x)))\n",
    "\n",
    "ri_ = rearrange(ri, 'b d n -> b (d n)')\n",
    "ri_prime_= rearrange(ri_prime, 'b a c -> b (a c)')\n",
    "\n",
    "zi = mlpf1(ri_)\n",
    "zi_prime = mlpf2(ri_prime_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3aea5a",
   "metadata": {},
   "source": [
    "## Loss Functions\n",
    "For pretraining, contrastive and denoising losses between a given data point, and its views generated by CutMix and mixup, are minimized.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img width=\"1000\" height=\"1000\" src=\"media/loss_functions.jpg\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2cc35b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contrastive loss:\n",
      " 42.96498489379883\n"
     ]
    }
   ],
   "source": [
    "def contrastive_loss(zi, zi_prime):\n",
    "    eps = 1e-7\n",
    "\n",
    "    zi_prod = einsum('a b, c b -> a c', zi, zi_prime)\n",
    "    zi_prod = zi_prod / 0.7\n",
    "\n",
    "    zi_exp = torch.exp(zi_prod)\n",
    "    zi_exp_sum = torch.sum(zi_exp, dim=-1, keepdim=True)\n",
    "\n",
    "    return -1.0 * torch.sum(torch.log(F.relu(torch.diag(zi_exp / zi_exp_sum)) + eps))\n",
    "    \n",
    "\n",
    "c_loss = contrastive_loss(zi, zi_prime)\n",
    "print(\"contrastive loss:\\n\", c_loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "41d2d8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def denoising_loss(xi, ri, ri_prime, config, cats, no_cat, no_num):\n",
    "\n",
    "    def clones(module, N):\n",
    "        \"Produce N identical layers.\"\n",
    "        return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "\n",
    "    mse = nn.MSELoss()\n",
    "    ce = nn.CrossEntropyLoss()\n",
    "\n",
    "    cat_mlps = nn.ModuleList()\n",
    "    for i in range(1, no_cat):\n",
    "        cat.mlps.append(\n",
    "            nn.Linear(config.n_embd, cats[i]))\n",
    "\n",
    "    num_mlp = nn.Sequential(\n",
    "        nn.Linear(config.n_embd, 1), nn.ReLU())\n",
    "\n",
    "    num_mlps = clones(num_mlp, no_num)\n",
    "\n",
    "    denoising_loss = 0.0\n",
    "    num_loss = 0.0\n",
    "    cat_loss = 0.0\n",
    "\n",
    "    for feat_idx in range(1, no_cat): # exlude [cls]\n",
    "        # get mlp projection for each categorical feature\n",
    "        ri_feat = cat_mlps[feat_idx - 1](\n",
    "            ri_prime[:, feat_idx, :].squeeze()) # BS x 1\n",
    "\n",
    "        xi_feat = xi[:, feat_idx] # BS x 1\n",
    "\n",
    "        cat_loss += ce(ri_feat.float(), xi_feat.long())\n",
    "        \n",
    "    for feat_idx in range(no_num):\n",
    "        idx = no_cat + feat_idx\n",
    "\n",
    "        # get the mlp for the feature\n",
    "        ri_feat = num_mlps[feat_idx](ri_prime[:, idx, :]) # BS x 1\n",
    "\n",
    "        xi_feat = xi[:, idx] # BS x 1\n",
    "\n",
    "        num_loss += mse(ri_feat.squeeze().float(), xi_feat.float())\n",
    "\n",
    "    return num_loss + cat_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "06c71d13",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3149371936.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn [19], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    print(f\"pre-training loss:\\n:\", L_pretraining = contrastive_loss(zi, zi_prime) + denoising_loss(x, ri, ri_prime, config, cats, no_cat, no_num); print(L_pretraining)\u001b[0m\n\u001b[1;37m                                                                                                                                                  ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "print(L_pretraining = contrastive_loss(zi, zi_prime) + denoising_loss(x, ri, ri_prime, config, cats, no_cat, no_num); print(L_pretraining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f08646e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
