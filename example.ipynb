{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "237ebba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "from einops import rearrange\n",
    "from typing import List, Dict, Union\n",
    "from argparse import Namespace\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import einsum\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "from operations.data import generate_dataset\n",
    "from operations.data import generate_dataloader\n",
    "from operations.embeds import Embedding\n",
    "from operations.model import NewGELU\n",
    "from operations.utils import generate_splits\n",
    "from operations.utils import preprocess\n",
    "from operations.utils import CutMix, Mixup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc038c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data\n",
    "data = pd.read_csv('data/creditcard.csv')\n",
    "\n",
    "# generate split indices\n",
    "sup_train_indices, val_indices, test_indices, ssl_train_indices = generate_splits(data.shape[0])\n",
    "\n",
    "# preprocess data\n",
    "df_proc, y_proc, no_num, no_cat, cats = preprocess(data.drop(columns=['Class']), data.Class, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2653caa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate train/val/test sets\n",
    "train_df, train_y = df_proc.iloc[sup_train_indices], y_proc.iloc[sup_train_indices]\n",
    "val_df, val_y = df_proc.iloc[val_indices], y_proc.iloc[val_indices]\n",
    "test_df, test_y = df_proc.iloc[test_indices], y_proc.iloc[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18937a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# where to store our train/val/test sets\n",
    "config.train_csv_path = 'data/train/target/train_targets.csv'\n",
    "config.val_csv_path = 'data/val/target/val_targets.csv'\n",
    "config.test_csv_path = 'data/test/target/test_targets.csv'\n",
    "config.train_y_csv = 'data/train/label/train_labels.csv'\n",
    "config.val_y_csv = 'data/val/label/val_labels.csv'\n",
    "config.test_y_csv = 'data/test/label/test_labels.csv'\n",
    "\n",
    "# save the preprocessed data\n",
    "train_df.to_csv(config.train_csv_path, index=False)\n",
    "train_y.to_csv(config.train_y_csv, index=False)\n",
    "\n",
    "val_df.to_csv(config.val_csv_path, index=False)\n",
    "val_y.to_csv(config.val_y_csv, index=False)\n",
    "\n",
    "test_df.to_csv(config.test_csv_path, index=False)\n",
    "test_y.to_csv(config.test_y_csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7db284d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader reads in files using their designated paths\n",
    "train_dataset, val_dataset, test_dataset = generate_dataset(\n",
    "                                            train_csv_path = config.train_csv_path,\n",
    "                                            val_csv_path = config.val_csv_path,\n",
    "                                            test_csv_path = config.test_csv_path,\n",
    "                                            train_y_csv_path = config.train_y_csv,\n",
    "                                            val_y_csv_path = config.val_y_csv,\n",
    "                                            test_y_csv_path = config.test_y_csv)\n",
    "\n",
    "# dictionary containing data paths that will be passed to the generate_dataloader class\n",
    "data_paths = {\n",
    "    \"train_csv_path\": config.train_csv_path,\n",
    "    \"val_csv_path\": config.val_csv_path,\n",
    "    \"test_csv_path\": config.test_csv_path,\n",
    "    \"train_y_csv_path\": config.train_y_csv,\n",
    "    \"val_y_csv_path\": config.val_y_csv,\n",
    "    \"test_y_csv_path\": config.test_y_csv\n",
    "}\n",
    "\n",
    "# prepare our train, validation, and test loaders\n",
    "train_loader, validation_loader, test_loader = generate_dataloader(train_bs=16, \n",
    "                                                                   val_bs=16, \n",
    "                                                                   num_workers=0, \n",
    "                                                                   data_paths=data_paths,\n",
    "                                                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "bd4354d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial configuration\n",
    "config.n_embd = 10\n",
    "config.no_num = no_num\n",
    "config.no_cat = no_cat\n",
    "config.cats = cats\n",
    "config.n_head = 2\n",
    "config.resid_pdrop = 0.8\n",
    "config.prob_cutmix = 0.3 # used in paper\n",
    "config.mixup_alpha = 0.2 # used in paper\n",
    "config.d_k = config.n_embd // config.n_head\n",
    "config.scale = config.n_head ** -0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd365e36",
   "metadata": {},
   "source": [
    "## Self Supervised Pre-Training\n",
    "<p align=\"center\">\n",
    "    <img width=\"500\" height=\"350\" src=\"media/media.jpg\">\n",
    "</p>\n",
    "\n",
    "SAINT implements contrastive pre-training, where the distance between two views of the same point is minimized while maximizing the distance between two different points. This strategy is coupled with denoising to perform pre-training on datasets with varied volumes of labeled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9e7ccfe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = next(iter(train_loader))[0] # (16, 31)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d16c232",
   "metadata": {},
   "source": [
    "The CutMix regularization strategy is used to augment samples in the input space, and mixup for samples in the embedding space. Specifically, mixup generates convex combinations of pairs of examples and their labels to regularize the NN to favor simple linear behaviour in-between training examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8e247d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_mix = CutMix(config.prob_cutmix)\n",
    "mix_up = Mixup(config.mixup_alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d20ac5",
   "metadata": {},
   "source": [
    "Continous and categorical features are projected into the higher dimensional embedding space before being passed through the transformer blocks. A seperate single fully-connected layer with a ReLU nonlinearity is used for each continous feature to project the 1-dimensional input into d-dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "a66d9848",
   "metadata": {},
   "outputs": [],
   "source": [
    "em_1 = Embedding(config.n_embd, config.no_num , config.no_cat, config.cats) # +1 to account for addition of <cls> token\n",
    "em_2 = Embedding(config.n_embd, config.no_num , config.no_cat, config.cats)\n",
    "\n",
    "# embed batch\n",
    "pi = em_1(x)\n",
    "# embed cutmixed batch\n",
    "pi_prime_em = em_2(cut_mix(x))\n",
    "# mixup embedded cutmixed batch\n",
    "pi_prime = mix_up(pi_prime_em)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35dca251",
   "metadata": {},
   "source": [
    "## SAINT Architecture\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img width=\"255\" height=\"200\" src=\"media/saint_block.jpg\">\n",
    "</p>\n",
    "Each layer has two attention blocks: one self-attention block, and one intersample attention block. The former is identical to the transformer block proposed by Vaswani et al., where the model takes in a sequence of feature embeddings and ouputs contextual representations of the same dimension. The latter uses intersample attention in lieu of self-attention, that being the only difference in architecture between the two blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "cefa3154",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    \"\"\" an unassuming Transformer block \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.to_qkv = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = nn.MultiheadAttention(config.n_embd, num_heads=config.n_head)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = nn.ModuleDict(dict(\n",
    "            c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd),\n",
    "            c_proj  = nn.Linear(4 * config.n_embd, config.n_embd),\n",
    "            act     = NewGELU(),\n",
    "            dropout = nn.Dropout(config.resid_pdrop),\n",
    "        ))\n",
    "        m = self.mlp\n",
    "        self.mlpf = lambda x: m.dropout(m.c_proj(m.act(m.c_fc(x)))) # MLP forward\n",
    "\n",
    "    def forward(self, x):\n",
    "        q, k, v = to_qkv(ln_1(x)).chunk(3, dim=-1)\n",
    "        attn_output, attn_mask = self.attn(q, k, v)\n",
    "        \n",
    "        x = x + attn_output\n",
    "        x = x + self.mlpf(self.ln_2(x))\n",
    "        return x\n",
    "    \n",
    "self_attn_block = SelfAttention(config)\n",
    "z_almost = self_attn_block(pi_prime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95933a28",
   "metadata": {},
   "source": [
    "Intersample attention computes attention over samples rather than features.\n",
    "<p align=\"center\">\n",
    "    <img width=\"1000\" height=\"1000\" src=\"media/intersample_attention.jpg\">\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "9dd734c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(query, key, value, dropout=None):\n",
    "    \"Compute 'Scaled Dot Product Attention'\"\n",
    "    # query: bs, n, embed_dim\n",
    "    # key: bs, n, embed_dim\n",
    "    # value: bs, n, embed_dim\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    \n",
    "    p_attn = F.softmax(scores, dim = -1)\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)            # bs , n , n\n",
    "    output = torch.matmul(p_attn, value)    # bs, n , embed_dim\n",
    "    return output, p_attn\n",
    "\n",
    "\n",
    "def intersample(query , key , value,dropout=None):\n",
    "    \"Calculate the intersample of a given query batch\" \n",
    "    #x , bs , n , d \n",
    "    b, h, n , d = query.shape\n",
    "    #print(query.shape,key.shape, value.shape )\n",
    "    query , key , value = query.reshape(1, b, h, n*d), \\\n",
    "                            key.reshape(1, b, h, n*d), \\\n",
    "                                value.reshape(1, b, h, n*d)\n",
    "\n",
    "    output, _ = attention(query, key ,value)  #1 , b, n*d\n",
    "    output = output.squeeze(0) #b, n*d\n",
    "    output = output.reshape(b, h, n, d) #b,n,d\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "class MultiHeadedIntersampleAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        \"Take in model size and number of heads.\"\n",
    "        super(MultiHeadedIntersampleAttention, self).__init__()\n",
    "        assert d_model % h == 0\n",
    "        # We assume d_v always equals d_k\n",
    "        self.to_qkv = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.linears = nn.ModuleList([copy.deepcopy(\n",
    "                                nn.Linear(d_model, d_model)) for _ in range(4)]\n",
    "                                    )\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "     \n",
    "    def forward(self, x):\n",
    "        \"Implements Figure 2\"\n",
    "        q, k, v = self.to_qkv(x).chunk(3, dim=-1)\n",
    "        nbatches = q.size(0)\n",
    "        \n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k \n",
    "        query, key, value = [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "             for l, x in zip(self.linears, (q, k, v))]\n",
    "        \n",
    "        # 2) Apply attention on all the projected vectors in batch. \n",
    "        x = intersample(query, key, value, \n",
    "                                 dropout=self.dropout)\n",
    "        \n",
    "        # 3) \"Concat\" using a view and apply a final linear. \n",
    "        x = x.transpose(1, 2).contiguous().view(nbatches, -1, self.h * self.d_k) # bs , n , d_model\n",
    "        return self.linears[-1](x)  # bs , n , d_model\n",
    "    \n",
    "intersample_attn_block = MultiHeadedIntersampleAttention(config.n_head, config.n_embd)\n",
    "z1 = intersample_attn_block(z_almost) + a_almost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861f6ba5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
